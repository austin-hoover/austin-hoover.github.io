---
title: Fine-tuning arguments
date: 2023-06-29
author: Austin Hoover
categories: [teleological arguments, fine-tuning, multiverse]
bibliography: references.bib
csl: ../american-physics-society.csl
draft: true
---

The universe is fine-tuned for life if the set of possible life-permitting universes is small relative to the set of possible universes. Many think the evidence of fine-tuning supports either the *multiverse hypothesis*, which posits an ensemble of worlds, or the *design hypothesis*, which posits an intelligent designer.^[The multiverse and design hypotheses are not necessarily incompatible.] Alternatively, some think fine-tuning requires no explanation at all. I assess these responses in this post.


## 1. Paley’s watch

Let’s first visit the classical biological design argument. Paley expressed the basic idea:

> “In crossing a heath, suppose I pitched my foot against a stone, and were asked how the stone came to be there; I might possibly answer, that, for anything I knew to the contrary, it had lain there forever: nor would it perhaps be very easy to show the absurdity of this answer. But suppose I had found a watch upon the ground, and it should be inquired how the watch happened to be in that place; I should hardly think of the answer I had before given, that for anything I knew, the watch might have always been there. ... There must have existed, at some time, and at some place or other, an artificer or artificers, who formed [the watch] for the purpose which we find it actually to answer; who comprehended its construction, and designed its use. ... Every indication of contrivance, every manifestation of design, which existed in the watch, exists in the works of nature; with the difference, on the side of nature, of being greater or more, and that in a degree which exceeds all computation.” [@Paley_1829]

Oppy [@Oppy_2006_arguing] has a compact critique of Paley’s argument: Paley’s inference to design stems from three observations: (i) the watch has a principal function, (ii) various parts of the watch have functions, and (iii) the materials from which the parts are constructed are well suited to the functions that those parts have. But it appears that Paley’s inference to design relies instead on the background knowledge that watches are not naturally produced. But it is not clear that this inference works for biological systems, for which we do not have the same background knowledge. The argument never gets off the ground.

An extension of the biological design argument focuses on “irreducibly complex” systems that would cease functioning if any of their parts were removed [@Behe_2003]. Behe claims that it would be impossible for an irreducibly complex system to arise from gradual changes since the predecessor of any irreducibly complex system would have to be irreducibly complex. And if irreducible complexity is impossible on naturalism, then irreducible complexity supports the design hypothesis.^[It seems that the proponent of the argument from irreducible complexity will want to identify the first irreducibly complex system with the first biological system. A broader definition would include non-biological systems, but theories of physics and chemistry plausibly explain the complexity of such systems. A narrower definition would exclude some early life forms, but evolutionary theory provides a plausible natural link between simple and complex biological systems. On the other hand, it is unknown how biological systems developed from chemical systems. But it is difficult to see how such a link could not exist.]

Modern scientific theories undercut biological design arguments. For example, the theory of biological evolution aims to provide a natural link between simple and complex biological systems. There are major gaps to fill in these theories, but it is plausible that the laws of physics, chemistry, and biology are all that are needed to connect the early universe to the present universe. This type of explanation is not complete, however. Note that we posit physical laws which link a prior state of the world to the present state. All prior states are explained by iteration: the state at time $t$ is explained by the laws and the state at time $t’$, where $t’ < t$; however, we will eventually arrive at either an initial state ($t = 0$) or an infinite regress of prior states ($t \rightarrow -\infty$). It is difficult to know what else to do if there is an initial state since our explanatory mechanism (laws + prior states) no longer works. It might be troubling if the initial state had no explanation, for then all subsequent states would lack an ultimate explanation. It is unclear whether an infinite explanatory regress solves this problem.^[These considerations demonstrate a relationship between cosmological and design arguments.] Furthermore, one might ask for an explanation of the physical laws. Deeper considerations such as these are central to the cosmic fine-tuning argument.


## 2. Cosmic fine-tuning

The universe would be fine-tuned for life if the set of possible life-permitting universes was much smaller than the set of possible universes. An immediate difficulty here is how to define the set of possible universes. The universe is characterized by states, laws, and constants. We can consider a *state* to be a snapshot of the universe at a single time; for example, in classical physics, a state could be the position and momentum of every particle $\{\mathbf{x}_i, \mathbf{p}_i\}$, or in quantum physics, a state could be the wavefunction $\Psi (\mathbf{x})$. Laws describe relationships between states and are not measured directly; they emerge as part of a theory to explain the available data. Finally, fundamental *constants* are empirically determined scalars, i.e., not determined by the proposed theory. We need to determine whether the constants, laws, and prior states could have been different and, if so, whether they are fine-tuned for life.

It is hard to say whether laws could have been different, but let’s suppose they could have been different and ask whether they are fine-tuned. Here we have an infinite-dimensional function space of possible laws. We could start by just removing the laws we know of, i.e., removing gravity or electromagnetism. It seems plausible from this thought experiment that both attracting and repelling forces are necessary for any complex structures to emerge, let alone biological systems. But we clearly cannot use the same analysis that we use for the constants — varying one or two at a time. Nonetheless, the laws are, in a certain sense, simple, elegant, and precise, leading to order and predictability rather than chaos. It is not hard to see how an argument for theism might flow from such considerations [@Wigner_1960, @Hildebrand_2022; @Cutter_2023]. 

Let’s fix the laws and consider variations in the constants and prior states of the universe; this problem is more tractable. I will let the “fine-tuning data” refer to any calculations of the sensitivity of life-friendly conditions to changes to the constants or prior states, as well as all measurements that support the theories used to perform these calculations. A comprehensive discussion of the fine-tuning data is found in [@Barnes_2012]. The SEP article on fine-tuning [@sep-fine-tuning] has a shorter list of apparently fine-tuned parameters which I repeat here.

* The strength of gravity relative to electromagnetism
* The strength of the strong force relative to the electromagnetic force
* The ratio of up quark mass to down quark mass
* The strength of the weak force
* The value of the cosmological constant
* The energy density in the early universe
* The fluctuation amplitudes in the early universe
* The entropy of the early universe [@Albert_2001; [@sep-statphys-statmech; @sep-time-thermo; @Earman_2006; @Wallace_2016; @Wallace_2017; @Robinson_2023]
 
In some cases, the relevant parameters could not change more than one part in $10^{50}$ (or some other enormous value), rending the probability of life (or any complex structures), given that the parameters were selected from a sufficiently wide uniform distribution, to be essentially zero. These calculations are striking but are also controversial. Consider the following issues [@Oppy_2006_arguing]. 

1. Future physical theories may have no fine-tuned parameters.^[On the other hand, all future theories could contain a nonzero number of fine-tuned parameters. Or future theories could predict a probability distribution for each parameter, rending the actual parameter values either very likely or very unlikely.]
2. Assuming a complete theory of physics is still fine-tuned, how should we assign probability distributions in the space of possible worlds?^[A uniform distribution seems appropriate given our ignorance, but who is to say the parameters were *actually* drawn from a uniform distribution?]
3. Assuming a uniform distribution is appropriate for each parameter, what range can the various parameters take? A finite range seems arbitrary, and there are problems defining a uniform distribution over an infinite range.
4. Are we exploring the multidimensional parameter space rather than varying one parameter at a time?

Because of the lack of clear answers to these questions, it is unclear how confident we should be that the universe is fine-tuned for life. Nonetheless, I assign a moderate credence to the claim that the universe is fine-tuned for life, partly because fine-tuning is accepted by a significant fraction of physicists and philosophers. Thus, it makes sense to assume fine-tuning exists and work out its implications. 


## 3. Responses to fine-tuning

We can map the responses to fine-tuning to the following responses to an all-sixes configuration of many dice at a casino table:

1. *Many Rolls*: There were probably many prior rolls.^[If we knew the roll was fair, then given our background knowledge, it would be reasonable to suppose that somebody rolled the dice many times until the all-sixes configuration occurred. Without our background knowledge, we get the Inverse Gambler’s Fallacy.]
2. *Many Tables*: There were probably many simultaneous rolls.
3. *Intentional Agent*: An intentional agent arranged the dice in the all-sixes configuration.
4. *Lucky Roll*: The dice were rolled once. Unlikely events happen occasionally.
5. *Brute Contingency*: The dice could have landed in a different configuration, but no explanation is needed for the observed configuration.
6. *Brute Necessity*: The dice could not have landed in a different configuration.

Here, the all-sixes configuration is analogous to a life-supporting universe. Response 2 corresponds to the *multiverse hypothesis*: that there are many universes, each with different laws and initial conditions. Response 3 corresponds to the *design hypothesis*: that an intentional agent selected the laws and initial conditions of the universe. Response 4 corresponds to the claim that the laws and initial conditions were unlikely, but that unlikely events do not necessarily require explanations; even if random, the selection process provides a sufficient explanation. Response 5 suggests that there is no deeper explanation *at all* for the way things are; it is a brute fact. Response 6 corresponds to the claim that the laws and initial conditions are fixed by metaphysical necessity.


```{mermaid}
%%| echo: false
%%| fig-width: 585px
%%| fig-align: center
flowchart TB
  A[Evidence of fine-tuning] 
  A --> B[Accept]
      B --> D[Explanation needed]
          D --> F[Design]
          D --> G[Multiverse]
          D --> H[Chance]
      B --> E[No explanation needed]
          E --> I[Brute contingency]
          E --> J[Necessity]
  A --> C[Reject]
```
<br>

I think we can rule out Response 1 (*Many Rolls*), which would correspond to a cyclic universe model in which the “initial conditions” are effectively randomly selected on each cycle. I have no idea if such a model exists. I also think we can rule out Response 2 (*Lucky Roll*) if any better explanations are available (since the chances involved are incredibly small).

There are different ways to tease out the degree to which the fact that our universe is life-supporting ($L$) supports a hypothesis $H$. On the Bayesian view, $E$ supports $H$ if
$$ 
\frac{Pr(H | L)}{Pr(\neg H | L)} = \frac{Pr(L | H)}{Pr(L | \neg H)}\frac{Pr(H)}{Pr(\neg H)} > 1.
$$ 

In addition to the fit to the data (likelihood) $Pr(L | H)$, we also need to consider the prior probability $Pr(H)$. Assigning priors is challenging when $H$ is a large-scale hypothesis such as theism, which could be affected by other evidence. Assigning likelihoods is also challenging; we must do our best to be honest about what our hypothesis predicts. But this equation is at least a guide to comparing hypotheses. 


### 3.1. No explanation needed

It could be that our universe, and the fact that the constants are fine-tuned for life, is a brute fact without any deeper explanation. For example, it could be that the constants could have been different, but there is nothing we can do to explain *why* they are what they are. Although all explanations must stop somewhere, I’m not a big fan of brute contingency. In my studies of cosmological arguments, I’ve become attracted to the idea that there is at least one necessary being — an initial or eternal matter-energy configuration, God, or something like that. This brings us to the possibility that the fine-tuned constants are fixed by metaphysical necessity. This sits better with me. A great discussion about necessary entities is in [@Rasmussen_Leon]. More on this later.
 


### 3.2. The multiverse hypothesis

A popular response to the fine-tuning data is to posit an ensemble of universes, each with different parameters. Such a hypothesis — call it $M$ — could render the probability of life ($P(L | M)$) quite high if the number of universes is large. The support for $M$ would be strong if the probability of life, given a single universe, were small ($P(L | \neg M) \ll 1$) and if the unconditioned probability of a multiverse $P(M)$ were reasonably large.

#### 3.2.1. The prior probability of a multiverse 

I am tempted to assign a *very* low prior probability to the multiverse hypothesis ($P(M) \ll 1$). It is not obvious that there is more than one universe; it is not even clear what that would mean. But surprisingly, multiverse hypotheses have crept into modern physics. In my understanding, part of the motivation for the multiverse hypothesis — in the context of cosmic fine-tuning — comes from String Theory + Inflation, which predicts a large number of vacuum states with different effective constants and laws.^[I think this is the $10^{500}$ number I always hear about.]  Each “universe” (in my understanding) is just an isolated space-time region; this is somewhat easier to stomach than a separate reality. String Theory is speculative at this point, so I’m not sure how much stock we should hold in this idea; nonetheless, it does not seem unreasonable to suppose that some future theory could predict a “multiverse” of effective constants and laws.^[There is also some motivation from outside of physics, from the modal realism of David Lewis, but I find that view to be crazy.]

On another front, there are worries that the multiverse leads to absurd consequences. For example, consider the problem of Boltzmann Brains: a random fluctuation is much more likely to generate a universe that consists of just me, complete with false memories and sensory inputs, rather than a billion-year-old universe. But it is unreasonable to believe I am such a fluctuation since it would imply that the physics used to conclude I was a fluctuation was false (just part of the fluctuation). That is a problem! However, since this is not an issue unique to the multiverse, perhaps we should let it be.[@Wall]. Finally, there are problems about how to think about probability in a multiverses [@ref].


#### 3.2.2. The chances of life in a multiverse

Even if we assign a reasonably high $P(M)$, there are potential problems estimating $P(L | M)$. We need to know something about the multiverse dynamics. It is not clear, a priori, whether all universes would be equally likely; it seems possible that some universes could be more likely than others, rendering life-permitting universes incredibly unlikely. Or the underlying multiverse theory could include yet more fundamental fine-tuned constants. 

Then we have problems arising from an *infinite* multiverse. Assuming all universes are equally likely, the probability that at least one universe is life-supporting grows with the number of universes. But if the number of universes is infinite, the probability is either 1 or undefined ($\infty / \infty$) [@Wall]. If the probability is 1, one might argue that $M$ is not predictive (everything happens somewhere). I am not bothered by $Pr(L | M) = 1$ if $M$ is well-motivated; otherwise, it appears to be an ad hoc, too-good-to-be-true solution to the fine-tuning problem.

Let’s put aside those worries and assume there is a large (maybe infinite) ensemble of universes, a small fraction of which support life. Still, some argue that while a multiverse increases the probability that *some* universe is life-permitting, it does not increase the probability that *this* universe is life-permitting. In other words, the multiverse hypothesis makes the same mistake as the Inverse Gambler’s Fallacy. In our dice rolling example, if there are many tables, the probability that *at least one table* rolls all sixes will increase with the number of tables, but the probability that *the fourth table* rolls all sixes does not depend on the number of tables. 

The above analogy doesn’t seem right. It is not as if we are sitting in a universe, waiting to see if it is life-permitting; rather, we can only find ourselves in a life-permitting universe. It is as if we were standing outside the casino and were only let in if at least one all-sixes configuration was rolled (and were then brought to an all-sixes table). In this new analogy, the probability that we find ourselves at an all-sixes table scales with the number of tables. This issue is somewhat complicated. Saad [@Saad_2023] takes great care to make the dice-rolling relevantly like our situation with respect to the fine-tuning data, concluding that “even once those complicating factors are taken into account, fine-tuning should boost our confidence in the existence of other universes.” 

In conclusion, the multiverse hypothesis could be a reasonable solution to the fine-tuning problem. I am not convinced by the Inverse Gambler’s Fallacy objection. However, there are serious problems that must be addressed — perhaps by a future, fleshed-out version of the hypothesis.

 
### 3.3. The design hypothesis

#### 3.3.1. What would God create? The Goldilocks hypothesis

#### 3.3.2. Why would God create a fine-tuned universe?

#### 3.3.3. Stalking-horse naturalism

#### 3.3.4. God and the multiverse

#### 3.3.5. The prior probability of a designer



## 4. Conclusion




## 5. Plans

Before moving on to arguments from evil, I plan to spend more time on teleological and cosmological arguments. In particular, I plan to spend some time on nomological arguments [@Hildebrand_2022; @Cutter_2023], arguments from psychophysical harmony [@Cutter_2022], and arguments from the 

 [psychophysical harmony](https://philarchive.org/rec/CUTPHA), and [the unreasonable effectiveness of mathematics](https://www.tandfonline.com/doi/abs/10.1080/14746700.2011.547001). These are all related to the fine-tuning argument. I also have not covered the [Thomistic cosmological argument]() defended by Edward Feser. Or I may skip to arguments from evil and come back to these at some point.





The contingency (cosmological) arguments and the design arguments are not independent. Views on modality, contingency, etc., are important! 


